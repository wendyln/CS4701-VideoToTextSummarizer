{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**This file allows one to run our system with a pre-loaded video and pre-trained image captioning model. This does not include our tests, performance benchmarks, model training, etc... for the various components.**\n",
        "\n",
        "*These can be found in video_to_frames.ipynb, Image_captioning_with_transformers_final.ipynb, and image_captions_to_story.py, respectively*"
      ],
      "metadata": {
        "id": "WtK3bOLyWQ4J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When running this file, make sure a video is loaded in the directory or runtime environment, and change the video variable in the last cell to the video file name"
      ],
      "metadata": {
        "id": "B7sqHSuAXnlH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI3TfcmV7rAR"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12jVnErKIL_U",
        "outputId": "c2285269-b46b-4346-8eb1-a2c1c35174e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: deeplake<4 in /usr/local/lib/python3.10/dist-packages (3.9.33)\n",
            "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.10/dist-packages (from deeplake<4) (1.26.4)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (from deeplake<4) (1.35.36)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from deeplake<4) (8.1.7)\n",
            "Requirement already satisfied: pathos in /usr/local/lib/python3.10/dist-packages (from deeplake<4) (0.3.3)\n",
            "Requirement already satisfied: humbug>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from deeplake<4) (0.3.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deeplake<4) (4.66.6)\n",
            "Requirement already satisfied: lz4 in /usr/local/lib/python3.10/dist-packages (from deeplake<4) (4.3.3)\n",
            "Requirement already satisfied: pyjwt in /usr/local/lib/python3.10/dist-packages (from deeplake<4) (2.10.1)\n",
            "Requirement already satisfied: aioboto3>=10.4.0 in /usr/local/lib/python3.10/dist-packages (from deeplake<4) (13.2.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from deeplake<4) (1.6.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from deeplake<4) (2.10.3)\n",
            "Requirement already satisfied: libdeeplake in /usr/local/lib/python3.10/dist-packages (from deeplake<4) (0.0.151)\n",
            "Requirement already satisfied: aiobotocore==2.15.2 in /usr/local/lib/python3.10/dist-packages (from aiobotocore[boto3]==2.15.2->aioboto3>=10.4.0->deeplake<4) (2.15.2)\n",
            "Requirement already satisfied: aiofiles>=23.2.1 in /usr/local/lib/python3.10/dist-packages (from aioboto3>=10.4.0->deeplake<4) (24.1.0)\n",
            "Requirement already satisfied: botocore<1.35.37,>=1.35.16 in /usr/local/lib/python3.10/dist-packages (from aiobotocore==2.15.2->aiobotocore[boto3]==2.15.2->aioboto3>=10.4.0->deeplake<4) (1.35.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from aiobotocore==2.15.2->aiobotocore[boto3]==2.15.2->aioboto3>=10.4.0->deeplake<4) (3.11.10)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.10/dist-packages (from aiobotocore==2.15.2->aiobotocore[boto3]==2.15.2->aioboto3>=10.4.0->deeplake<4) (1.17.0)\n",
            "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from aiobotocore==2.15.2->aiobotocore[boto3]==2.15.2->aioboto3>=10.4.0->deeplake<4) (0.12.0)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3->deeplake<4) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3->deeplake<4) (0.10.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from humbug>=0.3.1->deeplake<4) (2.32.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from libdeeplake->deeplake<4) (0.3.9)\n",
            "Requirement already satisfied: ppft>=1.7.6.9 in /usr/local/lib/python3.10/dist-packages (from pathos->deeplake<4) (1.7.6.9)\n",
            "Requirement already satisfied: pox>=0.3.5 in /usr/local/lib/python3.10/dist-packages (from pathos->deeplake<4) (0.3.5)\n",
            "Requirement already satisfied: multiprocess>=0.70.17 in /usr/local/lib/python3.10/dist-packages (from pathos->deeplake<4) (0.70.17)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->deeplake<4) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic->deeplake<4) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic->deeplake<4) (4.12.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.37,>=1.35.16->aiobotocore==2.15.2->aiobotocore[boto3]==2.15.2->aioboto3>=10.4.0->deeplake<4) (2.8.2)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.37,>=1.35.16->aiobotocore==2.15.2->aiobotocore[boto3]==2.15.2->aioboto3>=10.4.0->deeplake<4) (2.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->humbug>=0.3.1->deeplake<4) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->humbug>=0.3.1->deeplake<4) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->humbug>=0.3.1->deeplake<4) (2024.8.30)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore==2.15.2->aiobotocore[boto3]==2.15.2->aioboto3>=10.4.0->deeplake<4) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore==2.15.2->aiobotocore[boto3]==2.15.2->aioboto3>=10.4.0->deeplake<4) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore==2.15.2->aiobotocore[boto3]==2.15.2->aioboto3>=10.4.0->deeplake<4) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore==2.15.2->aiobotocore[boto3]==2.15.2->aioboto3>=10.4.0->deeplake<4) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore==2.15.2->aiobotocore[boto3]==2.15.2->aioboto3>=10.4.0->deeplake<4) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore==2.15.2->aiobotocore[boto3]==2.15.2->aioboto3>=10.4.0->deeplake<4) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore==2.15.2->aiobotocore[boto3]==2.15.2->aioboto3>=10.4.0->deeplake<4) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore==2.15.2->aiobotocore[boto3]==2.15.2->aioboto3>=10.4.0->deeplake<4) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.37,>=1.35.16->aiobotocore==2.15.2->aiobotocore[boto3]==2.15.2->aioboto3>=10.4.0->deeplake<4) (1.17.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.5.1+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.1->torchvision) (3.0.2)\n",
            "Requirement already satisfied: pycocoevalcap in /usr/local/lib/python3.10/dist-packages (1.2)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from pycocoevalcap) (2.0.8)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->pycocoevalcap) (3.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->pycocoevalcap) (1.26.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.17.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\n",
            "Requirement already satisfied: wordfreq in /usr/local/lib/python3.10/dist-packages (3.1.1)\n",
            "Requirement already satisfied: ftfy>=6.1 in /usr/local/lib/python3.10/dist-packages (from wordfreq) (6.3.1)\n",
            "Requirement already satisfied: langcodes>=3.0 in /usr/local/lib/python3.10/dist-packages (from wordfreq) (3.5.0)\n",
            "Requirement already satisfied: locate<2.0.0,>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from wordfreq) (1.1.1)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from wordfreq) (1.1.0)\n",
            "Requirement already satisfied: regex>=2023.10.3 in /usr/local/lib/python3.10/dist-packages (from wordfreq) (2024.9.11)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy>=6.1->wordfreq) (0.2.13)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes>=3.0->wordfreq) (1.3.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes>=3.0->wordfreq) (1.2.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from marisa-trie>=1.1.0->language-data>=1.2->langcodes>=3.0->wordfreq) (75.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install \"deeplake<4\"\n",
        "!pip install --upgrade transformers\n",
        "!pip install nltk\n",
        "!pip install torchvision\n",
        "!pip install pycocoevalcap\n",
        "!pip install --upgrade opencv-python opencv-python-headless\n",
        "!pip install wordfreq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ryHdwAy-IR_b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import deeplake\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "from torchvision import transforms\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import nltk\n",
        "import random\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part I : Video to Frames"
      ],
      "metadata": {
        "id": "qO0OKY2fWhew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_frames_with_opencv(video_path, output_dir, gap):\n",
        "    \"\"\"\n",
        "    Extracts and saves frames from a video at specified intervals.\n",
        "\n",
        "    Parameters:\n",
        "    - video_path (str): Path to the video file.\n",
        "    - output_dir (str): Directory to save the extracted frames.\n",
        "    - gap (int): Interval between frames to save (e.g., every 10th frame).\n",
        "\n",
        "    Returns:\n",
        "    - list of str: Paths to the saved frame images, or an empty list if the video cannot be opened.\n",
        "\n",
        "    The function captures frames from the specified video file, saving every nth frame defined by the gap parameter.\n",
        "    It logs the process, including the number of frames processed and saved. Errors in opening the video are also logged.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Cannot open video file {video_path}\")\n",
        "        return []\n",
        "\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    print(f\"Video FPS: {fps}, Total frames: {total_frames}\")\n",
        "\n",
        "    frame_count = 0\n",
        "    saved_count = 0\n",
        "    image_paths = []\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            print(f\"End of video or frame read failed at frame {frame_count}.\")\n",
        "            break\n",
        "\n",
        "        if frame_count % gap == 0:\n",
        "            image_path = os.path.join(output_dir, f\"frame_{saved_count*gap}.jpg\")\n",
        "            cv2.imwrite(image_path, frame)\n",
        "            image_paths.append(image_path)\n",
        "            saved_count += 1\n",
        "            print(f\"Saved frame {frame_count} to {image_path}\")\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    print(f\"Total frames processed: {frame_count}\")\n",
        "    print(f\"Total frames saved: {saved_count}\")\n",
        "    return image_paths"
      ],
      "metadata": {
        "id": "twSCKs_ZWoYy"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part II : Image Captioning"
      ],
      "metadata": {
        "id": "iBe5KL0CXZeS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbB6vD0HIUrF"
      },
      "source": [
        "## Preprocessing Data, Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhBYn8_kITKe",
        "outputId": "4619d4e3-e2b0-45cf-af3e-92c42c2cd2fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "-"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Opening dataset in read-only mode as you don't have write permissions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "|"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/activeloop/flickr30k\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hub://activeloop/flickr30k loaded successfully.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r \r\r\r"
          ]
        }
      ],
      "source": [
        "ds = deeplake.load('hub://activeloop/flickr30k')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "AAjqeTxtw3lI"
      },
      "outputs": [],
      "source": [
        "images = ds.image\n",
        "captions = ds.caption_0 ## NextSteps: currently only training on caption0, could include other set of captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "rz1B0m3LklBi"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "uTe2jKsQIc6_"
      },
      "outputs": [],
      "source": [
        "caption_max_length = 20\n",
        "vocab_size = len(tokenizer)\n",
        "\n",
        "def process_image(image):\n",
        "  \"\"\"\n",
        "  Image transformation functions\n",
        "  Resizes images to 224x224, converts to tensor, and normalizes\n",
        "  The normalization parameters are mean, SD of of r, g, b pixel values\n",
        "  \"\"\"\n",
        "  image_transform = transforms.Compose([\n",
        "      transforms.Resize((224, 224)),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "  ])\n",
        "  image = image.numpy()\n",
        "  image = Image.fromarray(image)\n",
        "  image = image_transform(image)\n",
        "  return image\n",
        "\n",
        "\n",
        "def process_caption(caption_text):\n",
        "  \"\"\"\n",
        "  Tokenizes captions\n",
        "  Returns tensor of tokenized captions, with a max length of 20,\n",
        "  padding until this length, and truncating if the caption is longer\n",
        "  The individual entries in the tensor are integers\n",
        "  \"\"\"\n",
        "\n",
        "  tokenized = tokenizer(caption_text,\n",
        "                        max_length=caption_max_length,\n",
        "                        padding='max_length',\n",
        "                        truncation=True,\n",
        "                        return_tensors='pt')\n",
        "\n",
        "  input_ids = tokenized['input_ids']\n",
        "  attention_mask = tokenized['attention_mask']\n",
        "  return input_ids\n",
        "\n",
        "def untokenize(tokenized_text):\n",
        "    \"\"\"\n",
        "    Untokenizes a tensor\n",
        "    Takes a tensor of integers, and according to tokenizer, returns\n",
        "    original words\n",
        "    \"\"\"\n",
        "    return tokenizer.decode(tokenized_text, skip_special_tokens=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQnNe4LLstIv"
      },
      "source": [
        "## Transformer-Based Image Captioning\n",
        "Idea from : https://www.tensorflow.org/text/tutorials/image_captioning\n",
        "\n",
        "Steps :  \n",
        "\n",
        "1. Feature Extraction of Image (CNN)\n",
        "\n",
        "2. Word Embedding : word + positional embedding\n",
        "\n",
        "3. Decode Layer : Self attention, Cross Attention, Feed Forward Neural Network\n",
        "\n",
        "4. Output : Post Processing of probability vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsWg0ulvstIv"
      },
      "source": [
        "## Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "EE1y6qkvstIv"
      },
      "outputs": [],
      "source": [
        "class CNN_feature_extraction(nn.Module):\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    Initializes convolutional neural network\n",
        "    \"\"\"\n",
        "    super(CNN_feature_extraction, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1).to(device) # (224,224,64)\n",
        "    self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1).to(device)# (224,224,128)\n",
        "    self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1).to(device)# (224,224,256)\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fc = nn.Linear(256 * 28 * 28, 512).to(device)\n",
        "    self.fc2 = nn.Linear(512, 256).to(device)\n",
        "    self.flatten = nn.Flatten()\n",
        "\n",
        "    self.maxPool = nn.MaxPool2d(2, 2).to(device)\n",
        "\n",
        "    self.batchNorm1 = nn.BatchNorm2d(64).to(device)\n",
        "    self.batchNorm2 = nn.BatchNorm2d(128).to(device)\n",
        "    self.batchNorm3 = nn.BatchNorm2d(256).to(device)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Forward pass of convolutional neural network\n",
        "    Note: Output is already flattened\n",
        "    \"\"\"\n",
        "    # for each layer, run convolutional layer, batch normalization layer,\n",
        "    # then maxPool layer\n",
        "    layer1 = self.maxPool(self.relu(self.batchNorm1(self.conv1(x)))) # batch_size, 64, 112, 112 (b, c , h, w )\n",
        "    layer2 = self.maxPool(self.relu(self.batchNorm2(self.conv2(layer1)))) # batch_size, 128, 56, 56\n",
        "    layer3 = self.maxPool(self.relu(self.batchNorm3(self.conv3(layer2)))) # batch_size, 256, 28, 28\n",
        "    x1 = self.flatten(layer3)  # 1, 200704 = 256*28*28\n",
        "    hidden = self.relu(self.fc(x1))\n",
        "    x = self.fc2(hidden) # 1, 256\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_r8yJ00GstIw"
      },
      "source": [
        "## Embedding Layer\n",
        "Embedding = Word + Positional Embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "A8m_72EpstIw"
      },
      "outputs": [],
      "source": [
        "## Hyperparameter\n",
        "embed_dim = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "VQkAyPg3stIw"
      },
      "outputs": [],
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self, vocab_size, max_length, dim):\n",
        "        \"\"\"\n",
        "        Combines token and positional embeddings to produce sequence embeddings for Transformers.\n",
        "        Input:\n",
        "          vocab_size: size of vocab. Used for token embedding\n",
        "          max_length: max length of caption. Used for positional embedding\n",
        "          dim: Dimension of embedding vectors. Dimension of both token and positional Embedding\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.pos_embedding = nn.Embedding(num_embeddings=max_length, embedding_dim=dim).to(device)\n",
        "        self.token_embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=dim, padding_idx=0).to(device) # ie padding_idx that = 0 will not be trained\n",
        "\n",
        "    def forward(self, caption):\n",
        "        \"\"\"\n",
        "          Input: Caption (batch, max_length = 20)\n",
        "          Output: Embedding (batch, max_length, dim)\n",
        "        \"\"\"\n",
        "        _, max_length = caption.shape\n",
        "\n",
        "        token_embed = self.token_embedding(caption)\n",
        "\n",
        "        # Create a positional array with length = caption length (20)\n",
        "        positional_indices = torch.arange(max_length, device=caption.device).unsqueeze(0).to(device) # 1, 20\n",
        "        position_embed = self.pos_embedding(positional_indices)\n",
        "\n",
        "        return token_embed + position_embed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kp28MUuMstIw"
      },
      "source": [
        "## Decoder\n",
        "Contains self attention, cross attention, feed forward neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "m9_MAb_mstIx"
      },
      "outputs": [],
      "source": [
        "## Hyperparameters\n",
        "num_heads = 1\n",
        "dropout =  0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zA3zmQZPstIx"
      },
      "source": [
        "### Self Attention Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "kA_HS9QEstIx"
      },
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Self-Attention mechanism for token embeddings. Capture dependencies between words.\n",
        "\n",
        "    Inputs:\n",
        "        embed_dim: Embedding dimension = embed_dim\n",
        "        num_heads: The number of attention heads. Each head learns different aspects of the relationships between tokens.\n",
        "        dropout: Dropout rate used to prevent overfitting\n",
        "    \"\"\"\n",
        "    def __init__(self,  num_heads = 1, embed_dim = embed_dim, dropout=0.1):\n",
        "        super(SelfAttention, self).__init__()\n",
        "\n",
        "\n",
        "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout).to(device)\n",
        "        self.layernorm = nn.LayerNorm(embed_dim).to(device)\n",
        "\n",
        "    def forward(self, x_token):\n",
        "        \"\"\"\n",
        "        input shape : (batch, max_length, dim)\n",
        "        output shape : (batch, max_length, dim)\n",
        "        \"\"\"\n",
        "        attn_output, attn_output_weights = self.attention(query=x_token, key=x_token, value=x_token)\n",
        "        x = x_token + attn_output  #Residual connection prevent vanishing grad\n",
        "        return self.layernorm(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md-8BKxPstIx"
      },
      "source": [
        "### Cross Attention Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "t5PLiKmqstIx"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Cross Attention between caption and image.\n",
        "\n",
        "    Input:\n",
        "        Caption Embedding : (batch, seq_length, embed_dim)\n",
        "        Image Feature Extraction : (batch, 256)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads=1, dropout=0.1):\n",
        "        super(CrossAttention, self).__init__()\n",
        "        self.mha = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout).to(device)\n",
        "        self.layernorm = nn.LayerNorm(embed_dim).to(device)\n",
        "\n",
        "    def forward(self, caption, image):\n",
        "        batch_size_c, seq_length, embed_dim = caption.shape\n",
        "        batch_size_i, image_embed = image.shape\n",
        "        assert batch_size_c == batch_size_i, \"Batch Dimension of image and caption does not match\"\n",
        "        assert image_embed == 256, \"Image Shape Incorrect\"\n",
        "        assert image_embed == embed_dim , \"Image dimension does not match Token dimension\"\n",
        "\n",
        "        # create the same image for seq_length\n",
        "        image_broadcasted = image.unsqueeze(1).repeat(1, seq_length, 1) # batch, seq_length, image_dim\n",
        "        attn_output, attention_scores = self.mha(query=caption, key=image_broadcasted, value=image_broadcasted)\n",
        "        caption = caption + attn_output  # Residual connection\n",
        "        self.last_attention_scores = attention_scores\n",
        "        return self.layernorm(caption)  #(batch, max_length, image_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkERxHVistIx"
      },
      "source": [
        "### Feed Forward Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "icGwvQ1rstIy"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_dim, dropout_rate=0.1):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(embed_dim, 2 * embed_dim).to(device),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2 * embed_dim, embed_dim).to(device),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        )\n",
        "        self.layernorm = nn.LayerNorm(embed_dim).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.layer(x).to(device)\n",
        "        return self.layernorm(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K51XiO-jstIy"
      },
      "source": [
        "### Decoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "LMKOn3_xstIy"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads=1, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.self_attention = SelfAttention(num_heads, embed_dim, dropout)\n",
        "        self.cross_attention = CrossAttention(embed_dim, num_heads, dropout)\n",
        "        self.ffnn = FeedForward(embed_dim, dropout)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        image, caption = inputs\n",
        "        caption_1 = self.self_attention(caption)\n",
        "        output_seq = self.cross_attention(caption_1, image)\n",
        "        out_seq = self.ffnn(output_seq)\n",
        "\n",
        "        self.last_attention_scores = self.cross_attention.last_attention_scores\n",
        "\n",
        "        return out_seq # batch, seq_length, embed_dim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99ZztJyjstIy"
      },
      "source": [
        "## PostProcessing\n",
        "\n",
        "1. Problem: Some tokens occur much more frequently than others in the dataset.\n",
        "Without adjustment, the model may develop a bias toward predicting these frequent tokens, regardless of context.\n",
        "Incorporate token frequency information in the logits using the bias term.\n",
        "For instance, less frequent tokens are given a higher weight, encouraging the model to predict them when appropriate.\n",
        "\n",
        "2. Problem: Some tokens, like [UNK] or [START], are placeholders or special tokens not intended for prediction.\n",
        "Solution:\n",
        "Add a large negative bias (-1e9) to these tokens' logits to make their probabilities effectively zero after softmax.\n",
        "This guarantees they are not predicted during decoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "2Hy7XuhJstIy"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "WtfkhRSEstIy"
      },
      "outputs": [],
      "source": [
        "class PostProcessing(nn.Module):\n",
        "    def __init__(self, embed_dim, tokenizer, banned_tokens=('[UNK]', '[PAD]', '[SEP]', '[CLS]', '[MASK]', 'the', 'a', 'in')):\n",
        "\n",
        "        super(PostProcessing, self).__init__()\n",
        "        self.vocab_size = tokenizer.vocab_size\n",
        "        self.tokenizer = tokenizer\n",
        "        self.fc = nn.Linear(embed_dim, out_features=tokenizer.vocab_size).to(device) ## Goal is to map to vocab size # batch, seq_length, vocab_size\n",
        "        self.banned_tokens = banned_tokens\n",
        "\n",
        "        self.bias = None\n",
        "\n",
        "    def adapt(self, dataset):\n",
        "        \"\"\"\n",
        "            vocab_dict: a dictionary to map vocab word to assigned index (from 0 to vocab_size)\n",
        "            counts : a dictionary holding frequency of each word(token), key: id value :freq\n",
        "        \"\"\"\n",
        "        counts = collections.Counter()\n",
        "        vocab_dict = {name: id for id, name in enumerate(self.tokenizer.vocab)} # map name to id\n",
        "\n",
        "\n",
        "        for tokens in tqdm(dataset):\n",
        "            counts.update(tokens.numpy().flatten())  # Update freq for token in the sentence\n",
        "\n",
        "        # Turn Frequency into an array of all vocabulary\n",
        "        # fill vocab array with freq\n",
        "        # array indices = token_id\n",
        "        counts_arr = np.zeros(shape=(self.vocab_size,))\n",
        "        indices = np.array(list(counts.keys()), dtype=np.int32)\n",
        "        counts_arr[indices] = list(counts.values())\n",
        "\n",
        "\n",
        "        for token in self.banned_tokens:\n",
        "            id = vocab_dict[token]\n",
        "            counts_arr[id] = 0\n",
        "\n",
        "        total = counts_arr.sum()\n",
        "        p = counts_arr / total\n",
        "        p[counts_arr == 0] = 1.0  # Prevent log(0)\n",
        "        log_p = np.log(p)  # log(1) == 0\n",
        "\n",
        "        entropy = -(log_p * p).sum()\n",
        "\n",
        "        # print(f\"\\nUniform entropy: {np.log(self.tokenizer.vocab_size):0.2f}\")\n",
        "        # print(f\"Marginal entropy: {entropy:0.2f}\")\n",
        "\n",
        "        self.bias = torch.tensor(log_p, dtype=torch.float32)\n",
        "        self.bias[counts_arr == 0] = -1e9  # Set large negative values for banned tokens\n",
        "\n",
        "    def forward(self, input):\n",
        "        input = input.float()\n",
        "        x = self.fc(input)\n",
        "        # x = x + self.bias.to(x.device)  # Ensure bias is on the same device as the input\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OemMeeAgstIy"
      },
      "source": [
        "## Image Captioner : Put it all together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "pdjR7aOgstIz"
      },
      "outputs": [],
      "source": [
        "banned_tokens=('[UNK]', '[PAD]', '[SEP]', '[CLS]', '[MASK]', 'the', 'a', 'in')\n",
        "\n",
        "class ImageCaption(nn.Module):\n",
        "    def __init__(self, tokenizer, vocab_size, num_layers=1, embed_dim=256, max_length=20, num_heads=2, dropout=0.1):\n",
        "        super(ImageCaption, self).__init__()\n",
        "\n",
        "        self.feature_extractor_model = CNN_feature_extraction()\n",
        "        self.embedding = Embedding(vocab_size , max_length, embed_dim)\n",
        "        self.decoder_layer = DecoderLayer(embed_dim)\n",
        "        self.post_processing_model = PostProcessing(256, tokenizer, banned_tokens = banned_tokens)\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        # self.decoder_layers = nn.ModuleList([\n",
        "        #     DecoderLayer(embed_dim)\n",
        "        #     for _ in range(num_layers)\n",
        "        # ])\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        image, caption = inputs\n",
        "\n",
        "        # Step 1: Extract Features\n",
        "        extracted_image = self.feature_extractor_model(image)\n",
        "\n",
        "        # Step 2: Embed the tokens\n",
        "        tokens = self.embedding(caption)\n",
        "\n",
        "        # Step 3: Decode\n",
        "        token_output = self.decoder_layer((extracted_image, tokens))\n",
        "\n",
        "        output = self.post_processing_model(token_output)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predicting"
      ],
      "metadata": {
        "id": "NyxM1xDz_zTf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "S8opyUOA27a7"
      },
      "outputs": [],
      "source": [
        "# checkpoint_path = \"./model_checkpoint/checkpoint_epoch_10.pth\"\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def predict_one(model, dataset, image, idx, device, max_length=20):\n",
        "    \"\"\"\n",
        "    Predict the caption for an image from the dataset at a given index.\n",
        "    Args:\n",
        "        model: The trained ImageCaption model.\n",
        "        dataset: The deeplake dataset containing images. (not needed anymore)\n",
        "        idx: The index of the image in the dataset. (not needed anymore)\n",
        "        device: The device (CPU or GPU) to run the model on.\n",
        "        max_length: Maximum length of the generated caption.\n",
        "    Returns:\n",
        "        The predicted caption as a string.\n",
        "    \"\"\"\n",
        "    # Load and preprocess the image\n",
        "    # image = dataset.image[idx].numpy()\n",
        "    # processed_image = process_image(image).unsqueeze(0).to(device)\n",
        "    image = image.to(device)\n",
        "\n",
        "    # Start the generation process\n",
        "    with torch.no_grad():\n",
        "        # Step 1: Extract image features\n",
        "\n",
        "        extracted_image = model.feature_extractor_model(image)\n",
        "\n",
        "        # Step 2: Start with the <START> token\n",
        "        captions = torch.full(\n",
        "            (1, 1), tokenizer.convert_tokens_to_ids(\"<START>\"),\n",
        "            dtype=torch.long, device=device\n",
        "        )\n",
        "\n",
        "        # Step 3: Autoregressively generate caption\n",
        "        for _ in range(max_length):\n",
        "            # Embed tokens\n",
        "            token_embeddings = model.embedding(captions)\n",
        "\n",
        "            # Decode with cross-attention\n",
        "            token_output = model.decoder_layer((extracted_image, token_embeddings))\n",
        "\n",
        "            # Post-process to predict the next token\n",
        "            logits = model.post_processing_model(token_output[:, -1, :])  # Use the last token's output\n",
        "            next_token = logits.argmax(dim=-1).unsqueeze(1)  # Select the most probable token\n",
        "\n",
        "            # Append the next token to the captions\n",
        "            captions = torch.cat([captions, next_token], dim=1)\n",
        "\n",
        "            # Stop if the <END> token is generated\n",
        "            if next_token.item() == tokenizer.convert_tokens_to_ids(\"<END>\"):\n",
        "                break\n",
        "\n",
        "    # Untokenize the generated tokens\n",
        "    return untokenize(captions.squeeze(0).tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part III: Caption to Story"
      ],
      "metadata": {
        "id": "jW33kxHjdoPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordfreq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLZFAFa5drIe",
        "outputId": "3d6be156-08bc-4e5a-a2c0-4d3f2f98b377"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wordfreq in /usr/local/lib/python3.10/dist-packages (3.1.1)\n",
            "Requirement already satisfied: ftfy>=6.1 in /usr/local/lib/python3.10/dist-packages (from wordfreq) (6.3.1)\n",
            "Requirement already satisfied: langcodes>=3.0 in /usr/local/lib/python3.10/dist-packages (from wordfreq) (3.5.0)\n",
            "Requirement already satisfied: locate<2.0.0,>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from wordfreq) (1.1.1)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from wordfreq) (1.1.0)\n",
            "Requirement already satisfied: regex>=2023.10.3 in /usr/local/lib/python3.10/dist-packages (from wordfreq) (2024.9.11)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy>=6.1->wordfreq) (0.2.13)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes>=3.0->wordfreq) (1.3.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes>=3.0->wordfreq) (1.2.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from marisa-trie>=1.1.0->language-data>=1.2->langcodes>=3.0->wordfreq) (75.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "import math\n",
        "from wordfreq import word_frequency\n",
        "import re\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2ls4cXghWlZ",
        "outputId": "c2d3c1ad-12f9-4143-e22f-9239a070a830"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def captions_to_story_pretrained(captions):\n",
        "    \"\"\"\n",
        "    Function to take in captions, and output a story (a summary of the\n",
        "    video, with overall insights)\n",
        "    \"\"\"\n",
        "    long_text = '.'.join(captions)\n",
        "    summarizer = pipeline(\"summarization\")\n",
        "    result = summarizer(long_text)\n",
        "    print (result)\n",
        "\n",
        "\n",
        "def preprocessing(captions):\n",
        "    \"\"\"\n",
        "    Takes in a list of captions (or just a long string of text), returns\n",
        "    1. tokenized matrix of words in each sentence (list of lists)\n",
        "    2. the actual sentences (list of strings)\n",
        "    \"\"\"\n",
        "    # handles strings, splitting into list\n",
        "    if type(captions) == str:\n",
        "        captions = re.split(r\"(?<=[.?!])\", captions)\n",
        "    # ensure every caption ends in a period\n",
        "    for i in range(len(captions)):\n",
        "        if len(captions[i]) == 0:\n",
        "          continue\n",
        "        if captions[i][-1] not in  \".?!\":\n",
        "            captions[i] = captions[i] + \".\"\n",
        "    paragraph = ' '.join(captions)\n",
        "    sentences = nltk.sent_tokenize(paragraph)\n",
        "    res = []\n",
        "    for sentence in sentences:\n",
        "        res.append(nltk.word_tokenize(sentence))\n",
        "    return res, sentences\n",
        "\n",
        "def build_frequency_table(tokens, stops):\n",
        "    \"\"\"\n",
        "    Builds frequency table of words in a sentence (a mapping of words\n",
        "    to the number of times they appear). Does not include \"stop\" words\n",
        "    (e.g. , or . or ! or ?)\n",
        "    \"\"\"\n",
        "    table = {}\n",
        "    for word in tokens:\n",
        "        if word not in stops:\n",
        "            to_add = word.lower()\n",
        "            if to_add in table:\n",
        "                table[to_add] += 1\n",
        "            else:\n",
        "                table[to_add] = 1\n",
        "    return table\n",
        "\n",
        "def build_frequency_matrix(sentences, stops):\n",
        "    \"\"\"\n",
        "    Builds frequency matrix. For each sentence, a mapping from words to the\n",
        "    number of times they appear in the setence. Excludes words in \"stops\" (\n",
        "    , or . or ! or ?)\n",
        "    \"\"\"\n",
        "    matrix = {}\n",
        "    for i in range(len(sentences)):\n",
        "        table = build_frequency_table(sentences[i], stops)\n",
        "        matrix[i] = table\n",
        "    return matrix\n",
        "\n",
        "def build_tf_matrix(matrix):\n",
        "    \"\"\"\n",
        "    Takes in word frequency matrix, outputs TF matrix (a matrix of\n",
        "    word probabilities in each sentence). Essentially, normalizing by the\n",
        "    number of words in each sentence, so the sum of each row is 1\n",
        "    \"\"\"\n",
        "    res = {}\n",
        "    for key, table in matrix.items():\n",
        "        tot = sum(table.values())\n",
        "        freq_table = {}\n",
        "        for word, count in table.items():\n",
        "            freq_table[word] = count/tot\n",
        "        res[key] = freq_table\n",
        "    return res\n",
        "\n",
        "def build_appearing_matrix(matrix):\n",
        "    \"\"\"\n",
        "    Takes in word frequency (or tf) matrix - works with either - and produces\n",
        "    a mapping from word to number of sentences it appears in. This is needed\n",
        "    to produce idf matrix\n",
        "    \"\"\"\n",
        "    appearances = {}\n",
        "    for key, sentence in matrix.items():\n",
        "        for word in sentence:\n",
        "            if word in appearances:\n",
        "                appearances[word] += 1\n",
        "            else:\n",
        "                appearances[word] = 1\n",
        "    return appearances\n",
        "\n",
        "def build_idf_matrix(freq_matrix, appearing_matrix):\n",
        "    \"\"\"\n",
        "    Takes in frequence of each word in each sentence (freq matrix), and the\n",
        "    number of sentences each word appears in (appearing_matrix) and produces\n",
        "    the idf matrix\n",
        "    \"\"\"\n",
        "    idf_matrix = {}\n",
        "    num_sentences = len(freq_matrix)\n",
        "    for key, sentence in freq_matrix.items():\n",
        "        idf_table = {}\n",
        "        for word in sentence:\n",
        "            idf_table[word] = math.log(num_sentences/appearing_matrix[word])\n",
        "        idf_matrix[key] = idf_table\n",
        "    return idf_matrix\n",
        "\n",
        "def build_tf_idf_matrix(tf_matrix, idf_matrix):\n",
        "    \"\"\"\n",
        "    Builds tf-idf matrix for tf-idf algorithm, using both tf and idf\n",
        "    matrices\n",
        "    \"\"\"\n",
        "    tf_idf_matrix = {}\n",
        "    for key, sentence in tf_matrix.items():\n",
        "        idf_sentence = idf_matrix[key]\n",
        "        tf_idf_table = {}\n",
        "        for word in idf_sentence:\n",
        "            tf_idf_table[word] = sentence[word] * idf_sentence[word]\n",
        "        tf_idf_matrix[key] = tf_idf_table\n",
        "    return tf_idf_matrix\n",
        "\n"
      ],
      "metadata": {
        "id": "rnZn52z8eAEv"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_scoring(tf_idf_matrix):\n",
        "    \"\"\"\n",
        "    Returns average tf-idf score of words in a sentence. To be used\n",
        "    when selecting sentences in the tf-idf algorithm\n",
        "    \"\"\"\n",
        "    scores = {}\n",
        "    for key, sentence in tf_idf_matrix.items():\n",
        "        avg_score = np.mean(list(sentence.values()))\n",
        "        scores[key] = avg_score\n",
        "    return scores\n",
        "\n",
        "def choose_sentences_tfidf(sentence_scores, threshold):\n",
        "    \"\"\"\n",
        "    Takes in sentences scores (as calculated by\n",
        "    tf-idf algo) and a threshold for inclusion (e.g. 1.3). Returns\n",
        "    sentences to be included in summary\n",
        "    \"\"\"\n",
        "    scores = list(sentence_scores.values())\n",
        "    average = np.mean(scores)\n",
        "    sd = np.std(scores)\n",
        "    res = []\n",
        "    for key in sentence_scores:\n",
        "        if sentence_scores[key] > (average + sd * threshold):\n",
        "            res.append(key)\n",
        "    return res\n",
        "\n",
        "def choose_sentences_tfidf_modified(sentence_scores, idf_matrix, threshold):\n",
        "    \"\"\"\n",
        "    Modified version of the tf-idf algorithm. Chooses sentences both according\n",
        "    to tf-idf, and sentences with relatively uncommon english words that appear\n",
        "    frequently in the text\n",
        "    \"\"\"\n",
        "    # first, find tf-idf sentences to include\n",
        "    scores = list(sentence_scores.values())\n",
        "    average = np.mean(scores)\n",
        "    sd = np.std(scores)\n",
        "    res = []\n",
        "    for key in sentence_scores:\n",
        "        if sentence_scores[key] > (average + threshold * sd):\n",
        "            res.append(key)\n",
        "    # now, find relatively unique words that appear frequently\n",
        "    # include their sentences in the summary as well\n",
        "    rare_words_used = set()\n",
        "    for key, sentence in idf_matrix.items():\n",
        "        average_idf = sum(sentence.values())/len(sentence)\n",
        "        for word in sentence:\n",
        "            if word not in rare_words_used:\n",
        "              if word_frequency(word, \"en\") < 5e-5 and sentence[word] < 0.8 * average_idf:\n",
        "                rare_words_used.add(word)\n",
        "                res.append(key)\n",
        "                break\n",
        "    res.sort()\n",
        "    return res"
      ],
      "metadata": {
        "id": "77YAqPXIeHsj"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_summary(sentences, lst):\n",
        "    \"\"\"\n",
        "    Takes in the actual sentences, and a list of indices to be included,\n",
        "    creates text summary\n",
        "    \"\"\"\n",
        "    if len(lst) == 0:\n",
        "        return \"\"\n",
        "    summary = \"\"\n",
        "    for key in lst:\n",
        "        summary += sentences[key]\n",
        "        summary += \" \"\n",
        "    return summary\n",
        "\n",
        "def print_caption(caption):\n",
        "    \"\"\"\n",
        "    Utility function fo printing captions\n",
        "    \"\"\"\n",
        "    if type(caption) == str:\n",
        "        print (caption)\n",
        "        return\n",
        "    for i in range(len(caption)):\n",
        "        if len(caption) == 0 or caption[i][-1] not in \".!?\":\n",
        "            caption[i] += \".\"\n",
        "    res = ' '.join(caption)\n",
        "    print (res)\n",
        "\n",
        "def full_pipeline(caption):\n",
        "    \"\"\"\n",
        "    Takes a caption (either string or list of strings) and returns it summary,\n",
        "    according to a modified version of the tf-idf algorithm\n",
        "    \"\"\"\n",
        "    tokens, sentences = preprocessing(caption)\n",
        "    # build relevant matrices\n",
        "    freq_matrix = build_frequency_matrix(tokens, \",.!?\")\n",
        "    tf_matrix = build_tf_matrix(freq_matrix)\n",
        "    appearing_matrix = build_appearing_matrix(freq_matrix)\n",
        "    idf_matrix = build_idf_matrix(freq_matrix, appearing_matrix)\n",
        "    tf_idf_matrix = build_tf_idf_matrix(tf_matrix, idf_matrix)\n",
        "    # score sentences according to tf-idf metrics and modifications\n",
        "    sentence_scores = sentence_scoring(tf_idf_matrix)\n",
        "    # choose sentences to include, write summary\n",
        "    to_include = choose_sentences_tfidf_modified(sentence_scores, idf_matrix, 0.5)\n",
        "    summary = write_summary(sentences, to_include)\n",
        "    return summary"
      ],
      "metadata": {
        "id": "ZIo8Ht2yeQOD"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part IV : Entire System"
      ],
      "metadata": {
        "id": "uUP9VEoth7N7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = \"try.mp4\" # change to video title!\n",
        "checkpoint_path = \"checkpoint_epoch_10.pth\" # whatever the final\n",
        "\n",
        "# video to frames\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = ImageCaption(tokenizer, vocab_size)\n",
        "model.load_state_dict(torch.load(checkpoint_path)['model_state_dict'])\n",
        "image_directory = \"extracted_images\"\n",
        "gap = 20\n",
        "extracted_images = extract_frames_with_opencv(video_path, image_directory, gap)\n",
        "\n",
        "# image retreival, processing, captioning\n",
        "image_files = [file for file in os.listdir(image_directory) if file.endswith(\".jpg\")]\n",
        "\n",
        "caption_list = []\n",
        "for file in image_files:\n",
        "  image = Image.open(os.path.join(image_directory, file))\n",
        "  image = image.convert('RGB')\n",
        "  transformation = transforms.ToTensor()\n",
        "  image_tensor = transformation(image)\n",
        "  image_tensor = image_tensor.permute(1, 2, 0)\n",
        "  image_tensor_fixed = image_tensor.to(torch.uint8)\n",
        "\n",
        "  processed_image = process_image(image_tensor_fixed).unsqueeze(0).to(device)\n",
        "  predicted_caption = predict_one(model, None, processed_image, -1, device)\n",
        "  # predicted_caption = predict(cnn_feature_extraction, embedder, decoder, post_processor, processed_test_image, device)\n",
        "  words = predicted_caption.split()\n",
        "  new_words = list(filter(lambda x : x not in \".!?\", words))\n",
        "  for i in range(len(new_words)):\n",
        "    if new_words[i][-1] in \".!?\":\n",
        "      new_words[i] = new_words[i][:-1]\n",
        "  caption_list.append(' '.join(new_words))\n",
        "  caption_list[-1] = caption_list[-1] + \".\"\n",
        "\n",
        "print (caption_list)\n",
        "\n",
        "print ()\n",
        "print (\"ORIGINAL TEXT IS: \\n\")\n",
        "print_caption(caption_list)\n",
        "print ()\n",
        "summary = full_pipeline(caption_list)\n",
        "print (\"SUMMARIZED TEXT IS: \\n\")\n",
        "print (summary)\n",
        "print ()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qE_Rqtyln6ag",
        "outputId": "6ad0bc4c-4b12-47fd-fa7c-da5277e485fd"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-65-68db35b78233>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(checkpoint_path)['model_state_dict'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video FPS: 25, Total frames: 216\n",
            "Saved frame 0 to extracted_images/frame_0.jpg\n",
            "Saved frame 20 to extracted_images/frame_20.jpg\n",
            "Saved frame 40 to extracted_images/frame_40.jpg\n",
            "Saved frame 60 to extracted_images/frame_60.jpg\n",
            "Saved frame 80 to extracted_images/frame_80.jpg\n",
            "Saved frame 100 to extracted_images/frame_100.jpg\n",
            "Saved frame 120 to extracted_images/frame_120.jpg\n",
            "Saved frame 140 to extracted_images/frame_140.jpg\n",
            "Saved frame 160 to extracted_images/frame_160.jpg\n",
            "Saved frame 180 to extracted_images/frame_180.jpg\n",
            "Saved frame 200 to extracted_images/frame_200.jpg\n",
            "End of video or frame read failed at frame 216.\n",
            "Total frames processed: 216\n",
            "Total frames saved: 11\n",
            "['a man in a turn, behind them is a barricade and behind that are people.', 'a man in a leash with a large puddle at a red tank top of a large building.', 'a man in a leash with a large puddle at a captain in a park and.', 'a man in a leash with a toy airplane is a captain in a park large.', 'a man in a leash with a large puddle at a brown dog in a bucking bull.', 'a man in a leash with a large puddle at a captain in a park and.', 'a man in a leash with a large puddle at a brown dog in a bucking bull.', 'a man in a leash with a large puddle at a brown dog in a bucking bull.', 'a man in a woman both wearing a partial stadium with many people, blue pants and a.', 'a man in a leash with a toy airplane in a brown dog in a bucking bull.', 'a man in a leash with a large puddle at a brown dog in a bucking bull.']\n",
            "\n",
            "ORIGINAL TEXT IS: \n",
            "\n",
            "a man in a turn, behind them is a barricade and behind that are people. a man in a leash with a large puddle at a red tank top of a large building. a man in a leash with a large puddle at a captain in a park and. a man in a leash with a toy airplane is a captain in a park large. a man in a leash with a large puddle at a brown dog in a bucking bull. a man in a leash with a large puddle at a captain in a park and. a man in a leash with a large puddle at a brown dog in a bucking bull. a man in a leash with a large puddle at a brown dog in a bucking bull. a man in a woman both wearing a partial stadium with many people, blue pants and a. a man in a leash with a toy airplane in a brown dog in a bucking bull. a man in a leash with a large puddle at a brown dog in a bucking bull.\n",
            "\n",
            "SUMMARIZED TEXT IS: \n",
            "\n",
            "a man in a turn, behind them is a barricade and behind that are people. a man in a leash with a large puddle at a red tank top of a large building. a man in a leash with a large puddle at a red tank top of a large building. a man in a woman both wearing a partial stadium with many people, blue pants and a. a man in a leash with a toy airplane in a brown dog in a bucking bull. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "F0eD4il8WPAq"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "qO0OKY2fWhew",
        "iBe5KL0CXZeS",
        "TbB6vD0HIUrF",
        "jW33kxHjdoPq"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}